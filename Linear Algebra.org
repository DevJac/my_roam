:PROPERTIES:
:ID:       0678CF24-88E4-4014-AC51-1316BF707906
:END:
#+title: Linear Algebra
* Linear Algebra is Our Most Reliable Tool
A fundamental aspect of linear algebra (that no one ever taught me in a class) is that non-linear problems are almost never analytically solvable (e.g. e^x= y is easily solved through logarithms, but even solving xe^x=y requires Lambert’s W function). Almost all interesting real world problems are non-linear to some extent, therefore, linear algebra is really the only tool we have to make progress on many difficult problems (e.g. through linear approximation and then applying techniques of linear algebra to solve the linear problem).

-- https://news.ycombinator.com/item?id=30870705
* Linear Algebra
** Types
*** Scalars
A real number, rational or irrational.
*** Vectors
A sequence of scalars.

Unless stated otherwise, we usually assume vectors are column vectors. A column vectors with 5 units would have dimension 5 x 1 (rows x columns), it is a "column vector" because there is one column.
**** Unit vectors
Unit vectors are often written with a hat: $$ \hat{v} $$

Divide any vector by its norm to make it a unit vector.
**** Operations
***** Transpose
Swaps dimensions. Columns become rows, and rows become columns.

A double transpose does nothing.

$$ \vec{v}^{TT} = \vec{v} $$

***** Addition
Element-wise addition. Equivalent to chaining vectors end to end and taking their resulting vector.

Added vectors must have the same dimensionality.

Commutative: $$ \vec{a} + \vec{b} = \vec{b} + \vec{a} $$

***** Subtraction
Similar to addition, except negate the vector being subtracted.

If we have x - y, the result can be interpreted as a vector going from the tip of y to the tip of x (except located at the origin).

An easy way to remember this is to think of scalar subtraction as 1-dimensional vectors. If we have 2 - 1, the result is positive, because 2 is more positive than 1, so the result points in the positive direction. The result points in the direction of the first operand.
***** Multiplication
****** Scaling
Element-wise multiplication / scaling.

Commutative: $$ a \cdot \vec{v} = \vec{v} \cdot a $$

****** Element-wise multiplication
This is often notated with a dot in a circle.

$$ a \odot b $$

****** Dot product (aka, inner product)
This is one of the most important operations in linear algebra. It is the basis of convolution, correlation, the Fourier transform, matrix multiplication, signal filtering, etc.

It is a single number that represents the relationship between two vectors. It is sometimes called the "scalar product" for this reason.

Vectors of the dot product must have the same dimensionality.

The dot product is the sum of each element-wise product.
******* Norm, length, variance
The dot product of a vector with itself is the norm-squared. The square root of norm-squared is the norm, or the length. If the vector is mean-centered, then norm-squared is the statistical variance.
******* Associative
The dot product is associative with scalar multiplication, but not with another vector dot product:

\begin{equation*}
n \cdot ( \vec{v} \cdot \vec{u} ) =
( n \cdot \vec{v} ) \cdot \vec{u}
\end{equation*}

\begin{equation*}
\vec{w} \cdot ( \vec{v} \cdot \vec{u} ) \neq
( \vec{w} \cdot \vec{v} ) \cdot \vec{u}
\end{equation*}

******* Angle between vectors / cos
The dot product is the cosine of the angle between two vectors times the the lengths of the two vectors:

dot(a, b) = cos(angle_between(a, b)) * norm(a) * norm(b)

Thus:

cos(angle_between(a, b)) = dot(a, b) / (norm(a) * norm(b))
******* Neat application: weighted sum
The dot product is a multiplication followed by a sum. This pattern can be used to calculate a weighted sum by taking the dot product of the data and the weights. If the weights sum to 1, then the result will be a weighted average.
******* Projection times length interpretation
If we have "a dot b" or $$ \text{a}^\text{T}\text{b} $$:

It is equivalent to the length of b projected onto a times the length of a. Or vice-versa.
****** Outer product
The dot product (aka, inner product) and outer product are just special cases of matrix multiplication.

The outer product does *not* require the vectors to have the same dimensions.

The outer product is a multiplication between a column vector and a row vector.

There are a few ways of looking at it: The column is replicated and then scaled by the elements of the row. Or, the row is replicated and then scaled by the elements of the column.
****** Cross product
An operation between two vectors with 3 elements each.

Similar to the dot product, but involves sin instead of cos.

Rarely used in data analysis, statistics, machine learning, or signal-processing.

Can be used to find a vector orthogonal to a plane.
*** Matrices
**** Size
A M x N matrix has M rows and N columns.

It's common to use "M x N" and rows comes before columns in the size: rows x column.

The size can also be viewed as either: (column length x column count) or (row count x row length).
**** Special matrices
***** Square / non-square
A square matrix has the same number of rows and columns.
***** Symmetric / asymmetric
A matrix is symmetric is it is equal to its own transpose. A symmetric matrix must be square.
****** Skew-symmetric
In a skew-symmetric matrix one triangle has the opposite sign as the other. This requires that the diagonal be all zeros.

A matrix is skew-symmetric if it is equal to the negative of its own transpose.
***** Identity
An identity matrix is square, with all zeros except for 1's on the diagonal.

It could more accurately be called the "multiplicative identity matrix" to distinguish it from the "addititve identity matrix" which would be all zeros.
***** Diagonal
A diagonal matrix has only zeros off the diagonal. It may have any values on the diagonal, but may only have zeros everywhere else.

Diagonal matrices can be rectangular. They must have zeros everywhere except for where the row index equals the column index (the diagonal).

A "hollow" matrix is the opposite, a hollow matrix must have zeros on the diagonal, but may have other values everywhere else.

An "anti-diagonal" goes from the top-right to the lower-left.
***** Augmented
An augmented matrix is the result of concatenating two or more matrices column-wise (the rows grow, but the length and values of the columns is preserved).

A vertical line is often placed in the resulting matrix to separate the two matrices.

Augmented matrices are used when solving systems of linear equations, among other things.
***** Triangular
Triangular matrices has all zeros in either the upper triangle, or lower triangle. The diagonal may have non-zero values. They may be rectangular.
***** Sparse / dense (full)
A sparse matrix consists of mostly zeros.

The opposite is a "dense" matrix, which consists of mostly non-zero values (also called a "full matrix").

In computation, a sparse matrix is often represented by listing the values of non-zero elements, rather than storing every element (most of which would be zero).
***** Orthogonal / orthonormal
Matrix Q is orthogonal if $$ \mathbf{Q}^\text{T}\mathbf{Q} = \mathbf{I} $$, that is, the transpose of an orthogonal matrix is its inverse.

If Q is square and orthogonal, then:

\begin{equation*}
  \mathbf{Q}^\text{T}\mathbf{Q} =
  \mathbf{QQ}^\text{T} =
  \mathbf{Q}^{-1}\mathbf{Q} =
  \mathbf{QQ}^{-1} =
  \mathbf{I}
\end{equation*}

In an orthogonal matrix:
1. all columns are orthogonal with every other column (their dot product is 0, 90 degree angles between them), and
2. every column vector is length 1.
***** Toeplitz / Hankel
In a Toeplitz matrix each row is a "rotation" of the row above.

#+begin_src
[[a, b, c, d],
 [d, a, b, c],
 [c, d, a, b],
 [b, c, d, a]]
#+end_src

A Hankel matrix rotates the other way. And may contain zeros in the lower right triangle.

#+begin_src
[[a, b, c, d],
 [b, c, d, 0],
 [c, d, 0, 0],
 [d, 0, 0, 0]]

[[a, b, c, d],
 [b, c, d, a],
 [c, d, a, b],
 [d, a, b, c]]
#+end_src
**** AtA / covariance matrix
AtA, that is, A transposed with itself has many interesting properties:

- It is a square matrix, even if A is rectangular.
- It is symmetric, even if A isn't.
- It is full-rank if A is full-column-rank.
- It is invertible if A is full-column-rank.
- It has the same row space as A.
- It has orthogonal eigenvectors.
- It is positive and (semi)definite.
- It has non-negative, real-valued eigenvalues.
- It is called a "covariance matrix" if A is a data matrix.
- It often looks pretty.
**** Operations
***** Addition / subtraction
Simple element-wise addition and subtraction. Matrices must be the same size, and the result is the same size.

Matrix addition is commutative: A + B = B + A
***** Scalar multiplication
Scalar-matrix multiplication is simple element-wise multiplication.

Matrix-scalar multiplication is commutative: n * A = A * n
***** Matrix multiplication
Matrix multipliation is not commutative: A times B does not equal B times A.

An A x B matrix multipled by a C x D matrix results in a A x D matrix, and B and C must be equal.
****** 4 perspectives on matrix multiplication
[[./images/Linear Algebra/matrix_multiplication_perspectives.png]]
******* Element perspective (all possible dot / inner products)
(row count × row length) × (column length × column count)

In AB, every element is the dot product of the corresponding row of A and column of B.

The rows in A are the same length as the columns in B and thus have dot products.

[[./images/Linear Algebra/matrix_multiplication.png]]
******* Layer perspective (sum of outer product layers)
(column length × column count) × (row count × row length)

AB is the sum of every outer product between the corresponding columns in A and rows in B.

The column count in A is the same as the row count in B, thus the columns and rows pair up exactly for the outer product operation. The outer product does not require vectors to be the same length.
******* Column perspective (weighted sums / linear combinations)
(column length × column count) × (column length × column count)

In AB, every column is a weighted sum of the columns in A; the weights come from the columns in B.

The weight count in the columns of B must match the column count in A.
******* Row perspective (weighted sums / linear combinations)
(row count × row length) × (row count × row length)

In AB, every row is a weighted sum of the rows in B; the weights come from the rows in A.

The weight count in the rows of A must match the row count in B.
***** Matrix division
Matrix division doesn't exist. The closest thing is a matrix inverse.
***** Frobenius dot product
Flatten both matrices into a vector and take their dot product.

Written:

$$ \langle \mathbf{A}, \mathbf{B} \rangle_\text{F} $$

It is equal to the trace of A transpose B:

\begin{equation*}
\langle \mathbf{A}, \mathbf{B} \rangle_\text{F} =
\text{tr}( \mathbf{A}^\text{T} \mathbf{B} )
\end{equation*}

Remember, the trace is the sum of the diagonal and anti-diagonal.
***** Matrix norms
There are many ways interpreting and measuring a matrix norm, here are a few ways:

Sum the square of all elements in the matrix, and take the square root of that. This is called the Frobenius norm or L2 norm.

(It is called the L2 norm, because you raise each element to the power of 2, sum them, and then raise the result to 1/2 (aka, the square root). There are other norms, L1 norm, L3 norm, etc.)
***** Euclidean distance between two matrices
Perform element-wise subtractions between the elements of two matrices, this gives a difference. Square the element-wise differences, sum them, and then take the square root. This is the Euclidean distance between the two matrices.
***** Shifting
Shifting a matrix means adding a (often small) scalar to its diagonal elements. This can transform a rank-deficient matrix into a full-rank matrix.
***** Distributing transpositions
\begin{equation*}
( \mathbf{AB} )^\text{T} =
( \mathbf{B}^\text{T} \mathbf{A}^\text{T} )
\end{equation*}
***** Making symmetric
****** Additive
If the matrix A is square, it can be made symmetric by:

(1 / 2) (A^T + A)
****** Multiplicative
Any matrix A can be made symmetric by:

$$ \mathbf{A}^\text{T} \mathbf{A} $$

- or -

$$ \mathbf{AA}^\text{T} $$
***** Matrix asymmetry index
Every square matrix can be formed by adding a symmetric matrix and a skew-symmetric matrix. Remember, skew-symmetric means symmetric but with opposite signs.

An asymmetry index can be found by dividing the skew-symmetric L2 norm (the non-symmetric part) by the original matrix norm (the full matrix).

Let's say A is our original matrix, and K is the skew-symmetric "layer" of it. Their relationship is:

\begin{equation*}
\mathbf{K} =
( \mathbf{A} - \mathbf{A}^\text{T} ) / 2
\end{equation*}

\begin{equation*}
\mathbf{A}_{\text{symmetric}} = A - K
\end{equation*}
***** Diagonal / trace
The diagonal of a matrix is a vector containing the elements on the diagonal of the matrix.

The trace of a matrix is the sum of all elements on its diagonal and anti-diagonal. A trace is only defined for square matrices.
**** Rank
Rank is an integer indicating the number of dimensions of information contained in the matrix. Rank is a property of the matrix, regardless of application or how the matrix is interpreted.

Rank is the number of linearly independent vectors in the matrix (either rows or columns). It is the number of dimensions spanned by the rows or columns of the matrix.

For a M x N size matrix, the maximum possible rank is the smaller of M or N:

\begin{equation*}
  rank_{max}(\mathbf{A}_{\text{M} \times \text{N}}) = min(\text{M}, \text{N})
\end{equation*}

A "full-rank" matrix has the maximum possible rank for its shape. A "rank-deficient" (or "reduced-rank") matrix the opposite, having less than the maximum rank for its shape. A "singular" matrix is a square and rank-deficient matrix.

"Full-column-rank" means that the rank equals the number of columns. Each of the columns is linearly independent. "Full-row-rank" similarly applies to rows.
***** Operation effect on rank
****** Scalar multiplication
Scalar multiplication cannot change the rank, because stretching the vectors will change change the dimension of their span.
****** Addition
If we have two matrices, A and B:

rank(A + B) <= rank(A) + rank(B)

The result, of course, still has a maximum rank limited by its size: two full-rank 3x3's cannot create a 3x3 rank 6.
****** Matrix multiplication
If we have two matrices, A and B:

rank(AB) <= min(rank(A), rank(B))

The result, of course, still has a maximum rank limited by its size.
****** Transposition
\begin{equation*}
rank(\mathbf{A}) =
rank(\mathbf{A}^\text{T}) =
rank(\mathbf{A}^\text{T}\mathbf{A}) =
rank(\mathbf{AA}^\text{T})
\end{equation*}
**** Determinant
Things to know about determinants:

1. Only square matrices have a determinant
2. The determinant is a single scalar value
3. Each matrix has only one determinant
4. The determinant is zero for rank-deficient / singular matrices, thus any linear dependencies between rows or columns will result in a 0 determinant. In other words, a determinant of 0 means the matrix "crushes" the space.
5. The determinant is the product of the eigenvalues
6. The determinant is the amount of scaling the matrix performs on areas / volumes in the space

Notation:

\begin{equation*}
  \det(\mathbf{A}) = |A| =
  \begin{vmatrix}
    1 & 2 \\
    3 & 4
  \end{vmatrix}
\end{equation*}

Transpose does not affect the determinant:

$$ \det(\mathbf{A}) = \det(\mathbf{A}^\text{T}) $$

Computing the determinant is hard. The effort required grows exponentially as the matrix does.
***** Computing the determinant
The general method for computing the determinant is demonstrated in this image:

[[./images/Linear Algebra/determinant_computation.png]]

The determinant of a triangular matrix (all zeros in the upper-right, or lower-left triangle) is simply the product of its diagonal. It's often easier to transform a matrix into echelon form and then simply take the product of the diagonal. But in doing so, you must keep track of the elementary matrices for the row reductions you perform.

If we have:

\begin{equation*}
  \mathbf{A} =
  \mathbf{R}_N
  \mathbf{R}_{...}
  \mathbf{R}_2
  \mathbf{R}_1
  \mathbf{B}
\end{equation*}

then det(A) is the product of all determinants for all R and B:

\begin{equation*}
  \det(\mathbf{A}) =
  \det(\mathbf{R}_N)
  \det(\mathbf{R}_{...})
  \det(\mathbf{R}_2)
  \det(\mathbf{R}_1)
  \det(\mathbf{B})
\end{equation*}

Thus, we can decompose a matrix into a triangular matrix (e.g., echelon form) and then compute the product of many trivial determinants.

Remember that det(AB) = det(A) det(B).

When decomposing a matrix to echelon form, if we discover that it is singular (the rows or columns are linearly dependent), then we know the determinant is zero.

In general, any changes to a matrix change the determinant. The easiest way to track these changes is with simple elementary matrices.
****** Specific effects of elementary row operations
- Swapping rows negates ("flips") the determinant.
- Adding or subtracting another row has no effect.
- Scaling a row scales the entire determinant.
- Matrix scalar multiplication is like scaling every row and has a compounded effect based on the number of rows (or columns, since the matrix must be square).
***** Triangular matrices
Triangular matrices have all zeros in their upper or lower triangle. (That is, upper-right or lower-left triangle.) The determinant of these matrices is the product of their diagonal.
**** Inverse
If AB = I, then B is the inverse of A.

\begin{equation*}
  \mathbf{AA}^{-1} = \mathbf{I}
\end{equation*}

To solve Ax = b using the inverse matrix:

\begin{align*}
  \mathbf{A}\text{x} &= \text{b} \\
  \mathbf{A}^{-1}\mathbf{A}\text{x} &= \mathbf{A}^{-1}\text{b} \\
  \mathbf{I}\text{x} &= \mathbf{A}^{-1}\text{b} \\
  \text{x} &= \mathbf{A}^{-1}\text{b}
\end{align*}

The inverse is, of course, invertible:

$$ (\mathbf{A}^{-1})^{-1} = \mathbf{A} $$

The order of inverting and transposing do not matter:

invert(transpose(A)) = transpose(invert(A))

The inverse of a symmetric matrix is also symmetric.

The inverse of a matrix is unique, there is only one.

Computing a matrix inverse is numerically unstable and care must be taken when doing so on a computer. It is best to avoid computing with matrix inverses if possible.
***** Matrices with a null space have no inverse
The null space of a matrix is the vectors that it "crunches" down to zero. Crunching several different vectors into a single zero value destroys information. That information cannot be recovered.

Thus, any matrix that has a null space does not have an inverse.
***** Computing inverse with MCA
The inverse of A is:

(1 / det(A)) * adjugate(A)
****** Minor matrix
The minor matrix is a matrix of minors. Each element is replaced with its minor. The minor of an element is the determinant of the matrix that remains after removing the row and column containing that element.
****** Cofactor matrix
The minor matrix is element-wise-multiplied by a checkerboard matrix of +1's and -1's, with +1's along the diagonal; this produces the cofactor matrix, or rather a matrix of cofactors.
****** Adjugate matrix (aka, adjoint matrix)
The adjugate matrix is the transpose of the cofactor matrix.
***** Computing inverse with row reduction
To find the inverse of A, augment A with I, and then reduce to reduced row echelon form.

\begin{equation*}
  rref([\ \mathbf{A}\ |\ \mathbf{I}\ ]) =
  [\ \mathbf{I}\ |\ \mathbf{A}^{-1}\ ]
\end{equation*}

If you cannot reduce the augmented matrix to RREF form, then it does not have an inverse.

We use row reduction to solve systems of equations and Ax = b, this is similar. In this case we are solving AX = I.
***** Left / right inverse
If Ax = y, and A is rectangular, then x is transformed into a different dimension and becomes y.

A might have a left-inverse, a left-inverse is the only possible inverse it may have (given it's rectangular). I can remember it might have a left-inverse (rather than a right-inverse) because only the left side is free to attach an inverse to. I cannot attach the inverse to x. I cannot rearrange A and x because matrix multiplication is not commutative (AB does not equal BA). If an inverse exists at all, it must exist on the left.

It is not guaranteed that a rectangular matrix has a left or right inverse. It must be either full-row-rank or full-column-rank.
****** Why full rank is necessary for a one-sided inverse
A rank-deficient 3 x 2 matrix is a good example of why full-rank is necessary for rectangular matrices to have a one-sided inverse.

A rank-deficient 3 x 2 matrix must have a rank of 1 (or maybe 0). Let's say A is a 3 x 2 in the following examples.

In xA = y, A is transforming 3D x into 2D y. This will never be reversible.

In Ax = y, A is transforming 2D x into 3D y. This could be reversible as long as A doesn't crunch the 2 dimensions of x. Because x is 2D, A needs at least 2 linearly independent column basis vectors to preserve the dimensions of x. Yet A does not, A is rank-deficient, which means it does not have the full-column-rank required to preserve the dimensions of x. A is going to crunch the dimensions of x in an irreversible way. Thus, rank-deficient matrices cannot have even a one-sided inverse. Their failure to have full-rank means they cannot preserve all the dimensions they will be transforming.
****** Left inverse
The left inverse of A is:

\begin{equation*}
  ( \mathbf{A}^\text{T} \mathbf{A} )^{-1}
  \mathbf{A}^\text{T} =
  \mathbf{L}_\text{inv}
\end{equation*}

\begin{equation*}
  \mathbf{L}_\text{inv} \mathbf{A} = \mathbf{I}
\end{equation*}

Only tall matrices have a left inverse; that is, only matrices with more rows than columns have a left inverse.
****** Right inverse
The right inverse of A is:

\begin{equation*}
  \mathbf{A}^\text{T}
  ( \mathbf{AA}^\text{T} )^{-1} =
  \mathbf{R}_\text{inv}
\end{equation*}

\begin{equation*}
  \mathbf{R}_\text{inv}
  \mathbf{A} =
  \mathbf{I}
\end{equation*}

Only wide matrices have a right inverse; that is, only matrices with more columns than rows have a right inverse.
**** QR decomposition
A matrix can be decomposed such that A = QR, where Q is a orthogonal matrix and R is the residual (the left-over computation required to get back to A).

Q can be found using Gram-Schmidt orthogonalization. If Q is not full-rank, often more vectors are added until it is full-rank. The rank of Q may exceed the rank of A, but R will have the same rank as A.

Remember that if Q is orthogonal, then its transpose is also its inverse, thus it's trivial to solve for R in A = QR:

\begin{align*}
  \mathbf{A} &= \mathbf{QR}
\\
  \mathbf{QR} &= \mathbf{A}
\\
  \mathbf{Q}^\text{T}\mathbf{QR} &=
  \mathbf{Q}^\text{T}\mathbf{A}
\\
  \mathbf{R} &=
  \mathbf{Q}^\text{T}\mathbf{A}
\end{align*}

***** Gram-Schmidt orthogonalization
The basic idea is that for each vector, we must subtract off the component in the direction of each previous vector. Here is the process for a set of vectors {v1, v2, ..., vn}:

    Set u1 = v1.
    Set u2 = v2 - proj_u1(v2).
    For each additional vector vk (k > 2), set uk = vk - proj_u1(vk) - proj_u2(vk) - ... - proj_uk-1(vk).

Where proj_u(v) represents the projection of vector v onto the direction of vector u, defined as:

proj_u(v) = ((v⋅u) / (u⋅u)) * u

This procedure will give you a set of vectors {u1, u2, ..., un} which span the same space as {v1, v2, ..., vn}, but are orthogonal to each other.

Then normalize each vector uk to create the orthonormal set {q1, q2, ..., qn}.

Note: This algorithm assumes that the input vectors {v1, v2, ..., vn} are linearly independent. If they are not, the Gram-Schmidt process needs to be adjusted to handle that case.

The Gram-Schmidt algorithm is numerically unstable and care must be taken when implementing it on a computer. QR decomposition is probably already implemented in a better way.
***** Using QR to make inverses more numerically stable
Because the inverse of Q is so trivial (just a transpose), sometimes it's worthwhile to store, "ship around", and worth with QR rather than A. We still have to calculate the inverse of R, but R is a triangular matrix which is easier to work with.
*** Complex numbers
Complex numbers have a real and an imaginary component. They are written 1+2i or 3-4j.

0+1i = i = sqrt(-1)
i^2 = -1
**** Conjugate
The conjugate of a complex number negates (flips) the imaginary component. It is indicated by a bar or superscript asterisk:

$$ \text{z} = \text{a} + \text{b}i $$

$$ \bar{\text{z}} = \text{z}^* = \text{a} - \text{b}i $$
**** Addition / subtraction
These work as expected. Add the real parts together, add the imaginary parts together.
**** Multiplication
Multiplication is more complicated. We have to "FOIL" the two operands:

(a + bi)(c + di)
(ac + adi + bic + bidi)
(ac + adi + bci + bdi^2)
Remember i^2 = -1, thus:
(ac + adi + bci - bd)
(ac - bd + adi + bci)
(ac - bd) + (ad + bc)i
**** Division
Division is even more complicated:

\begin{align*}
\frac{ z }{ w } &=
\frac{ \text{a} + \text{b}i }{ \text{c} + \text{d}i } \\ &=
\frac{ (\text{c} - \text{d}i)(\text{a} + \text{b}i) }{ (\text{c} - \text{d}i)(\text{c} + \text{d}i) } \\ &=
\frac{ (\text{c} - \text{d}i)(\text{a} + \text{b}i) }{ \text{c}^2 - \text{d}^2 i^2 } \\ &=
\frac{ (\text{c} - \text{d}i)(\text{a} + \text{b}i) }{ \text{c}^2 + \text{d}^2 } \\ &=
\frac{ (\text{ac} + \text{bc}i - \text{ad}i - \text{bd}i^2) }{ \text{c}^2 + \text{d}^2 } \\ &=
\frac{ (\text{ac} + \text{bc}i - \text{ad}i + \text{bd}) }{ \text{c}^2 + \text{d}^2 } \\ &=
\frac{ (\text{ac} + \text{bd}) + (\text{bc} - \text{ad})i }{ \text{c}^2 + \text{d}^2 }
\end{align*}

Remember i^2 = -1.

Notice the denominator becomes real valued.
**** Hermitian transpose, dot product, matrix
The Hermitian transpose transposes the matrix, and also replaces all complex numbers with their conjugate.

It is denoted z^H instead of z^T.

A Hermitian dot product is (z^H)w instead of (z^T)w.

A Hermitian matrix satisfies the equation A = A^H.
** Concepts
*** "Linear combination"
It's called a /linear combination/ because it's a sum, a /combination/, of many terms, each of which has been scaled in a /linear/ fashion.
*** Vector space / linear space
A vector space is any set of objects for which addition and scalar multiplication are defined and follow these rules:

- Additive inverse: a + (-a) = 0
- Associativity: (a + b) + c = a + (b + c)
- Commutativity: a + b = b + a
- Additive identity: a + 0 = a
- Multiplicative identity: 1a = a
- Distributivity: (a + b)(x + y) = ax + ay + bx + by
- Closed under addition and scalar multiplication. This means that any linear combination of vectors in the space results in another valid vector of the space.
- Contains the zeros vector.
*** Vector subspace
A subspace is the span of all points you can reach by stretching and combining (scaling and adding) a set of vectors.
**** Number of subspaces in an ambient space
A 3D /ambient/ space will have:

1 0-dimensional subspace at the origin, the span of the zero vector.
Infinity 1-dimensional (line) subspaces.
Infinity 2-dimensional (plane) subspaces.
1 3D subspace that is the same as the ambient space.

Note, that's 4 subspace dimensions possible from a 3D space. The middles are infinite, the ends are 1. For an N-dimensional space, there are N+1 possible subspaces.
**** Vector space vs vector subspace
The difference between a vector space and a subspace is contextual. If, in context, we are talking about a smaller space of a larger space, then it is, naturally, called a subspace.

Every subspace is a vector space. And every vector space is a subspace of itself, technically, and also of all larger spaces.
*** Span
The span of a set of vectors is all the points that can be reached by a linear combination (a "weighted sum") of the vectors in that set.

A span is very similar to a space. A space is more of a noun, and a span is more of a verb, or more of a property of the vectors.

Vectors have a span, and are in a space. Vectors span a subspace.
*** Linear independence
A set of vectors is linearly /independent/ if each vector contributes to the span of the set.

There's a few different ways of saying this:
- The dimension of the span of the set equals the number of vectors in the set.
- None of the vectors could be created by a linear combination of the other vectors.
- A weighted sum of the vectors cannot produce 0 (excluding the trivial case of all weights being 0).

If a set of vectors is not linearly independent, then they are linearly /dependent/.

Linear independence is a property that a set of vectors may possess. It is not a property of individual vectors but a property of a set of vectors.
*** Basis
A basis for a subspace is a set of vectors that are linearly independent and can span the space.

There are infinitely many sets of basis vectors for any space, except the 0 space. Any linearly independent set of vectors is a basis of the space they span.

If the set of basis vectors were not linearly independent, the the coordinates of any point in the space would be unclear. That is, there would be multiple combinations of basis vectors that could result in the same vector.
*** Column space / row space
The column space of a matrix is the space spanned by its columns. In Ax = y, y is in the column space of A.

The notation for column space is C(A). The following is an example of notation and a true statement about the column space:

$$ C(\textbf{A}) = C(\textbf{AA}^\text{T}) $$

Remember: rank(A) = rank(A') = rank(A'A) = rank(AA') where A' is A transpose (the ' is transpose).
**** Is v in C(A)?
Is vector v in the column space of A?

To find out, augment A with the column v (concat vector v onto matrix A), then compare the matrix rank with and without v. If the rank is the same then v must be in the column space of A, because if v were outside the column space of A, then adding it to A would increase the rank.

If A is full-rank, then we can answer this question without knowing v, because A spans the entire ambient space.
**** Row space
The row space of A is denoted R(A).

$$ R(\mathbf{A}) = R(\mathbf{A}^\text{T} \mathbf{A}) $$

Row space is very similar, except the order of operands may have to change. For example, instead of Ax = y, we might have xA = y (y would be in the row space).
**** Dimensionality reduction
C(A) = C(AA') and R(A) = R(A'A)

AA' or A'A may have fewer dimensions but still span the same column or row space, and thus can be used for dimensionality reduction.
*** Null space
The null-space of a matrix is indicated N(A). It is the subspace containing all vectors v that satisfy Av = 0.

The vectors in the null-space, if multiplied by A, will become zero.

The null-space is a property of matrix A, it is the subspace of vectors that A will "crush" if we multiply by A.

Solutions for v in Av = 0 are the "right null-space", because v is to the right. There is also a "left null-space", a subspace of solutions for vA = 0.

In Ax = y, y is in the column space. However, x is not in the column space because it has a different dimension (length) than the columns.

\begin{equation*}
C(\mathbf{A}) \cup
N(\mathbf{A}^\text{T})
\text{ spans } \mathbb{R}^M
\end{equation*}

\begin{equation*}
R(\mathbf{A}) \cup
N(\mathbf{A})
\text{ spans } \mathbb{R}^N
\end{equation*}

N(A) is the "null space", or "right null space", N(A') is the "left null space". (The ' denotes transposition.)

dim(C(A)) + dim(N(A')) = M

dim(R(A)) + dim(N(A)) = N

And remember: rank(A) = dim(C(A)) = dim(R(A))
*** Orthogonal vectors / subspaces / matrices
**** Orthogonal vectors
Two vectors are orthogonal if they meet at a 90 degree angle.
**** Orthogonal subspaces
Two subspaces are orthogonal if every vector in subspace A is orthogonal to every vector in subspace B.

The notation is:

$$ \textbf{A} \perp \textbf{B} $$

The two subspaces must share the same ambient space, because the length of vectors in both spaces must be equal.
**** Orthogonal matrices / orthonormal
Matrix Q is orthogonal if $$ \mathbf{Q}^\text{T}\mathbf{Q} = \mathbf{I} $$, that is, the transpose of an orthogonal matrix is its inverse.

If Q is square and orthogonal, then:

\begin{equation*}
  \mathbf{Q}^\text{T}\mathbf{Q} =
  \mathbf{QQ}^\text{T} =
  \mathbf{Q}^{-1}\mathbf{Q} =
  \mathbf{QQ}^{-1} =
  \mathbf{I}
\end{equation*}

In an orthogonal matrix:
1. all columns are orthogonal with every other column (their dot product is 0, 90 degree angles between them), and
2. every column vector is length 1.
*** Ax = b and Ay = 0
These two equations are simple yet profound. It is no understatement to say that:

Most people learn linear algebra because they want to know how to solve these equations.
**** Ax = b
***** Does it have a solution?
Is b in the column space of A? If so, then there is a solution.
***** If not, then what is the closest approximation to an exact solution?
For this, we look for a new b that is as close as possible to the original b, and is in the column space of A.
**** Ay = 0
We are especially interested in a shifted version of A:

$$ (\mathbf{A} - \lambda\mathbf{I})y = 0 $$

This is related to eigenvectors and eigenvalues.
*** Systems of equations
Systems of equations can be represented as a matrix. A system of equations may have zero, one, or infinite solutions.
**** Converting a system of equations to matrices
\begin{align*}
\left
\begin{cases}
  3x + 4y &= 5 \\
  2x - y &= 0
\end{cases}
\right\}
\end{align*}

The above system of equations can be expressed using matrix multiplication:

\begin{equation*}
\begin{bmatrix}
  3 & 4 \\
  2 & -1
\end{bmatrix}
\begin{bmatrix}
  x \\
  y
\end{bmatrix}
=
\begin{bmatrix}
  5 \\
  0
\end{bmatrix}
\end{equation*}
**** Row reduction
We can do several things to the rows (or columns) of a matrix:

1. Scale rows
2. Reorder rows
3. Add multiples of other rows

All of these are reversible operations.

These linear transformations can be expressed as a series of matrix multiplications.

\begin{equation*}
  \mathbf{A} =
  \mathbf{R}_N
  \mathbf{R}_{...}
  \mathbf{R}_2
  \mathbf{R}_1
  \mathbf{B}
\end{equation*}

Either A or B may be viewed as the original matrix. The point is that there is a series of transformations required to go from A to B.

Atomic row reduction operations may be expressed as elementary matrices. Elementary matrices are invertible. The product of several elementary matrices is invertible. In A = RB, if R is invertible, then R(A) = R(B).

Row reduction does not change the rank of the matrix (the dimensions of the row space and column space do not change). Row reduction does not change the row space, but it may change the column space.
**** Echelon form / reduced row echelon form
If the lower left triangle is 0, then the matrix is in echelon form. The first non-zero value of each row must be further and further to the right.

Reduced row echelon form goes further and manipulates the matrix until all pivots are 1 and are the only non-zero value in their column. Columns without pivots may have several non-zero values.
***** Computing echelon form / reduced row echelon form
There are an infinite number of possible echelon forms. One form can be found by the "LU" decomposition. Python and Julia have a ~lu~ function for this.

In Julia the rref of the RowEchelon package can computer the reduced row echelon form. The algorithm to do so is sensitive, so be careful. In Python, you must use SymPy.
***** Pivots
When a matrix is in echelon form, the pivots are the first non-zero values of each row.

The number of pivots is the rank of the matrix.
***** Interpreting reduced row echelon form
****** No solutions
\begin{equation*}
\begin{bmatrix}
  1 & -2 & 0 \\
  0 &  0 & 1
\end{bmatrix}
\end{equation*}

This reduced row echelon form indicates that there are no solutions to the corresponding system of equations. The last line is equivalent to 0 = 1, which has no solution.
****** One solution
\begin{equation*}
\begin{bmatrix}
  1 & 0 & 2 \\
  0 & 1 & 3
\end{bmatrix}
\end{equation*}

This indicates that x = 2 and y = 3.
****** Many solutions
\begin{equation*}
\begin{bmatrix}
  1 & 2 & 4 \\
  0 & 0 & 0
\end{bmatrix}
\end{equation*}

This indicates that there are infinite many solutions. The first line is the only meaningful equations: x = 2y + 4.
*** Projections
**** Projections to a line / vector
proj_a(b) is the projection of vector b onto vector a. The result is a vector that is co-linear with a, and is the closest point on a to b.

\begin{equation*}
  proj_\text{a}(\text{b}) =
  \frac
  { \text{a}^\text{T}\text{b} }
  { \text{a}^\text{T}\text{a} }
  \text{a}
\end{equation*}

Remember that $$ \text{a}^\text{T}\text{b} $$ is the dot product of a and b.
***** Derivation
We are trying to find a point on line a that is closest to point b. The point closest to b can be represented by scaling a; we will scale a by x, and thus will call the point on a that is closet to b xa.

We know that the vector from xa to b will be orthogonal to a.

We now know enough to solve for x. Encode what we know as an equation and solve for x:

\begin{align*}
  \text{a}^\text{T}(\text{b} - \text{xa}) &= 0
\\
  \text{a}^\text{T}\text{b} -
  \text{a}^\text{T}(\text{xa}) &= 0
\\
  \text{a}^\text{T}\text{b} -
  \text{xa}^\text{T}\text{a} &= 0
\\
  \text{xa}^\text{T}\text{a} &=
  \text{a}^\text{T}\text{b}
\\
  \text{x} &=
  \frac
  {\text{a}^\text{T}\text{b}}
  {\text{a}^\text{T}\text{a}}
\end{align*}

Remember, in the last equation above, both sides evaluate to scalars.

Also remember that x is only the scaling factor of a; x is not the closest point to b, xa is the closest point to b. To reach our final equation, multiply both sides by a:

\begin{equation*}
  \text{xa} =
  \frac
  {\text{a}^\text{T}\text{b}}
  {\text{a}^\text{T}\text{a}}
  \text{a}
\end{equation*}

xa is the closest point to b.
**** Decomposing a vector into parallel and orthogonal components
Let's decompose vector b into two vectors, one that is parallel to a, and on that is orthogonal to a.

The parallel component we will call $$ \text{b}_{\,||\text{a}} $$.

The orthogonal component we will call $$ \text{b}_{\perp\text{a}} $$.

\begin{align*}
  \text{b} &=
  \text{b}_{\,||\text{a}} +
  \text{b}_{\perp\text{a}}
\\
  \text{b}_{\,||\text{a}} &=
  proj_{\text{a}}(\text{b})
\\
  \text{b}_{\perp\text{a}} &=
  \text{b} -
  \text{b}_{\,||\text{a}}
\end{align*}

**** Projections to a plane / subspace
Problem: What is the closest point in a subspace to a given point in the ambient space?

We have:
- A subspace spanned by the columns of matrix A, the column space of A, C(A).
- A point b in the ambient space, which may or may not be in the subspace.

What point in C(A) is closest to b? Let's call this point x. How do we find x?

We know that a vector from x to b must be orthogonal to C(A). Remember that the dot product of two orthogonal vectors is 0. Thus, the dot product of the vectors to b and x must be 0. We now have an equation, and x is the only unknown.

Remember that $$ \mathbf{A}^\text{T}\mathbf{B} $$ is the dot product of A and B.

Remember that $$ (\text{b} - \mathbf{A}\text{x} ) $$ is a vector from Ax to b (but without location, or located at the origin). We will find x to be the closest point to b, the components of x will be a linear combination of the columns in A which serve as the basis for the subspace.

Write what we know as an equation and solve for x:

\begin{align*}
  \mathbf{A}^\text{T} (\text{b} -
  \mathbf{A}\text{x} ) &= 0
\\
  \mathbf{A}^\text{T}\text{b} -
  \mathbf{A}^\text{T}\mathbf{A}\text{x} &= 0
\\
  \mathbf{A}^\text{T}\mathbf{A}\text{x} &=
  \mathbf{A}^\text{T}\text{b}
  && \text{This is similar to \mathbf{A}x = b}
\\
  (\mathbf{A}^\text{T}\mathbf{A})^{-1}
  \mathbf{A}^\text{T}\mathbf{A}\text{x} &=
  (\mathbf{A}^\text{T}\mathbf{A})^{-1}
  \mathbf{A}^\text{T}\text{b}
\\
  \text{x} &=
  (\mathbf{A}^\text{T}\mathbf{A})^{-1}
  \mathbf{A}^\text{T}\text{b}
\end{align*}

Compare to proj_a(b). Notice that proj_a(b) has AtA in the denominator; we cannot perform matrix division, but we can apply the inverse of AtA, which is what we've done here.
*** Least-squares / normal equation
Remember, one of the fundamental problems of linear algebra is solving Ax = b for x.

If b is not in the column space of A, then we can find the closest point in C(A) by solving:

\begin{align*}
  \mathbf{A}\text{x}
  &=
  \text{b}
\\
  \mathbf{A}^\text{T}\mathbf{A}\text{x}
  &=
  \mathbf{A}^\text{T}\text{b}
\\
  (\mathbf{A}^\text{T}\mathbf{A})^{-1}
  \mathbf{A}^\text{T}\mathbf{A}\text{x}
  &=
  (\mathbf{A}^\text{T}\mathbf{A})^{-1}
  \mathbf{A}^\text{T}\text{b}
\\
  \text{x}
  &=
  (\mathbf{A}^\text{T}\mathbf{A})^{-1}
  \mathbf{A}^\text{T}\text{b}
\end{align*}

This is using the left-inverse to solve for x.

This is equivalent to:
- Finding the point in C(A) nearest to b
- Projecting b onto C(A)
- Performing row reduction on an augmented matrix [ AtA | Atb ], the result will be [ I | x ]
- Finding the x that minimizes Ax - y (using calculus or other optimization techniques)

\begin{equation*}
  \mathbf{A}^\text{T}\mathbf{A}\text{x}
  &=
  \mathbf{A}^\text{T}\text{b}
\end{equation*}

The above equation is called the "normal equation".
*** Eigendecomposition
"Eigendecomposition is the process of revealing the matrix's deepest secrets."

Eigendecomposition is defined only for square matrices; it gives us eigenvalues (which are scalars) and eigenvectors.

Each M x M matrix has a set of M eigenvalues and M associated eigenvectors.

$$ \mathbf{A}\text{v}
  = \lambda\text{v}
  = \text{v}\lambda $$

Given Av = λv, v is the eigenvector and λ is the eigenvalue.

This means that any matrix of any size has the effect of scaling certain vectors. It's remarkable that, even if you have a 1000 x 1000 matrix, its effect on certain vectors is nothing more than scaling, nothing more than the effect of a single scalar.

There are M eigenvalues and eigenvectors, if we arrange them in a matrix we get:

$$ \mathbf{AV} = \mathbf{V\Lambda} $$

where $$\mathbf{V}$$ contains the eigenvectors as columns, and $$\Lambda$$ (an upper-case lambda) is a diagonal matrix of eigenvalues. Putting the eigenvectors into a matrix give them an ordering, but remember that we choose the ordering, and any ordering of the eigenvectors is valid (as long as they remain associated with their corresponding eigenvalue).
**** Finding eigenvalues
We can rework the equation for eigen-things into a solvable form.

\begin{align*}
  \mathbf{A}\text{v}
  &=
  \lambda\text{v}
\\
  \mathbf{A}\text{v}
  &=
  \lambda\mathbf{I}\text{v}
\\
  \mathbf{A}\text{v} -
  \lambda\mathbf{I}\text{v}
  &=
  0
\\
  (
    \mathbf{A} -
    \lambda\mathbf{I}
  )
  \text{v}
  &=
  0
\end{align*}

v is a non-zero vector which must be "crunched" to zero. A matrix that "crunches" v will have a determinant of zero. Now we have an equation involving the determinant we can solve.

\begin{equation*}
  |
    \mathbf{A} -
    \lambda\mathbf{I}
  |
  =
  0
\end{equation*}

As usual, determinant calculations are difficult to do by hand. The algebra works out such that we will have a Mth degree polynomial to solve, which will have M solutions.
**** Finding eigenvectors
When finding the eigenvalue, we eliminated vector v from the equation, because we knew that the matrix that multiplied v must have a determinant of zero (otherwise, it couldn't crunch v to zero).

Now that we know λ, we can put v back and solve for v.

\begin{equation*}
  (
    \mathbf{A} -
    \lambda\mathbf{I}
  )
  \text{v}
  &=
  0
\end{equation*}

v is in the nullspace of (A - λI), the above equation is essentially the definition of nullspace. There are infinite values of v that make this equation true, an infinite number of vectors in the nullspace. Pick one, that is an eigenvector.

To make things easy to work with, we often pick eigenvectors with a unit length of 1.
**** The matrix diagonal / diagonalization
Λ is the diagonal of matrix A, it is a diagonal matrix of A's eigenvalues. The relationship between A and Λ is given by these equations:

\begin{align*}
  \mathbf{AV}
  &=
  \mathbf{V\Lambda}
\\
  \mathbf{A}
  &=
  \mathbf{V\Lambda V}^{-1}
\\
  \mathbf{V}^{-1}\mathbf{AV}
  &=
  \mathbf{\Lambda}
\end{align*}

V is used as a change of basis in the latter two equations.

It is sometimes easier to perform calculations on the diagonal of a matrix, and then transform it back into A. If we have a function F, then sometimes:

$$ \text{F}(\mathbf{A})\ \mathbf{V} = \mathbf{V}\ \text{F}(\mathbf{\Lambda}) $$

Some matrices are not diagonalizable. They will have zero eigenvalues.
**** Repeated eigenvalues
Distinct eigenvalues always have distinct eigenvectors, but what about repeated eigenvalues?

Repeated eigenvalues have associated eigenspaces which *may* have more dimensions than one, but not necessarily. It's possible for repeated eigenvalues to have corresponding eigenvectors that are all within a 1-dimensional line, thus being no "better" than a distinct eigenvalue and its pair.

When an eigenvalue is repeated, the number of times it is repeated is the /algebraic multiplicity/ of that eigenvalue. The /geometric multiplicity/ of an eigenvalue is the dimension of the associate eigenspace, which is equivalent to the nullspace of (A - λI).

The geometric multiplicity is less-than-or-equal-to the algebraic multiplicity. The geometric multiplicity cannot be zero, since if an eigenvalue exist, the associated eigenvector(s) must span at least one dimension.

For a matrix to be diagonalizable, the geometric multiplicity must equal the algebraic multiplicity, for all eigenvalues.
**** Complex eigen-things
Real-valued matrices can have complex eigenvalues and eigenvectors, both will come in conjugate pairs.
**** Eigendecomposition of symmetric matrices
Symmetric matrices are nice to work with when doing eigendecomposition:
1. Symmetric matrices have orthogonal eigenvectors.
2. The eigen-things will be real valued.
**** Zero valued eigenvalues
If a matrix has a zero-valued eigenvalue, then it is singular / rank-deficient.

The number of zero-valued eigenvalue does not mean much, only whether or not there is one.

The determinant is a product of all eigenvalues, and thus a single zero-valued eigenvalue results in a zero determinant, a rank-deficient matrix.

Also, if we have Av = λv, and we know one of the eigenvalues is zero, then λ = 0, and we have Av = 0. We know that there is a solution for v, and thus A has a nullspace, and thus A is rank-deficient.

Also, if there is a v that solves Av = 0, then the matrix has a nullspace and is rank-deficient. In such a case there must be a eigenvalue of 0, because Av = λv = 0 when λ = 0.
**** Eigenlayers
The equation

$$ \mathbf{A} = \mathbf{V \Lambda V}^{-1} $$

can be interpreted as docomposing A into a seires of eigenlayers. Each eigenvalue has an associated layer, and the sum of all layers equals A.

To see this, remember the outer product interpretation of matrix multiplication.

Each layer is column i from V, times eigenvalue i, outer product row i.

Sum all these layers and you will have A.
*** Singular Value Decomposition (SVD)
Singular value decomposition is similar to eigendecomposition. Eigendecomposition can be interpreted as SVD applied to a square matrix.

$$ \mathbf{A} = \mathbf{U \Sigma V}^\text{T} $$

$\mathbf{A}$ (M x N, its shape is M x N) is the matrix to be decomposed, it can be square or rectangular.

$\mathbf{U}$ (M x M) is the /left singular vectors matrix/. It provides an orthonormal basis for $\mathbb{R}^M$, which includes the column space and left null space.

$\mathbf{\Sigma}$ (M x N) is the /singular values matrix/. It is a diagonal matrix and all values are real and non-negative (possibly zero).

$\mathbf{V}$ (N x N) is the /right singular vectors matrix/. It provides an orthonormal basis for the row space and null space. The term in the equation is $\mathbf{V}^\text{T}$, thus plain old $\mathbf{V}$ contains the singular vectors in its /columns/.
**** Relation to the 4 matrix spaces
[[./images/Linear Algebra/svd_4_spaces.png]]
**** Finding the SVD
To solve the SVD, replace A with AtA:

\begin{align*}
  \mathbf{A} &=
  \mathbf{U \Sigma V}^\text{T}
  \\
  \mathbf{A}^\text{T}\mathbf{A} &=
  (\mathbf{U \Sigma V}^\text{T})^\text{T}
  (\mathbf{U \Sigma V}^\text{T})
  \\
  \mathbf{A}^\text{T}\mathbf{A} &=
  (\mathbf{V \Sigma^\text{T} U^\text{T}})
  (\mathbf{U \Sigma V}^\text{T})
  \\
  \mathbf{A}^\text{T}\mathbf{A} &=
  \mathbf{V \Sigma^\text{T} U^\text{T}}
  \mathbf{U \Sigma V}^\text{T}
  \\
  \mathbf{A}^\text{T}\mathbf{A} &=
  \mathbf{V \Sigma^\text{T}}
  \mathbf{\Sigma V}^\text{T}
  \\
  \mathbf{A}^\text{T}\mathbf{A} &=
  \mathbf{V \Sigma^2 V^\text{T}}
  \\
  (\mathbf{A}^\text{T}\mathbf{A})
  \mathbf{V}
  &=
  \mathbf{V \Sigma^2}
\end{align*}

Remember V and U are orthonormal, so their transverse is also their inverse. This is how we eliminated UtU.

The final equation is very similar to eigenvalues, and we know how to solve those.
**** SVD layers and their importance
Remember the "layer" interpretation of matrix multiplication, that matrix multiplication is a sum of many outer-product layers. Given AB this interpretation is that AB is formed by summing many outer-products between the columns of A and the rows of B.

The columns of A and rows of B are often arbitrary and we don't know the relative importance of the various layers.

SVD give us similar layers, and additionally their relative importance.

Given A = UEV. E is a sorted diagonal matrix (by convention). Set E aside and we have A = UV. By the outer-product interpretation, the columns of U form outer-product layers with the rows of V, and then are scaled by terms of E. All together they form A.

This gives us many layers of A. The first SVD layer contains the most prominent patterns of A, and so on by decreasing significance.

Each layer is rank-1, because each layer is the result of a single outer-product. The SVD allows us to create low-rank approximations of A.

This is useful in noise reduction, data compression, feature selection, data analysis, etc.
* Book
** Linear Algebra: Theory, Intuition, Code by Mike X Cohen
*** Reading Log
<2023-05-25> Finished Chapter 3
<2023-05-31> Finished Chapter 6
<2023-06-08> Finished Chapter 9
<2023-06-11> Finished Chapter 12
<2023-06-15> Finished Chapter 15

<2023-06-16> I reached the chapter on SVD then stalled. I have a hard time seeing this being useful to me in the near future. Perhaps I will pick it up again later. A new edition of Linear Algebra Done Right comes out this year and I might try that book.
